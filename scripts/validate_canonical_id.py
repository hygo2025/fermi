"""
Script de Valida√ß√£o e An√°lise do Canonical Listing ID

Este script ajuda a validar a qualidade dos clusters criados pela
estrat√©gia de Item Canonicalization e fornece insights sobre:
- Redu√ß√£o de esparsidade
- Distribui√ß√£o de tamanho de clusters
- Impacto no hist√≥rico de intera√ß√µes

Uso:
    spark-submit scripts/validate_canonical_id.py
    
    # Ou via Python
    python scripts/validate_canonical_id.py
"""

import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from src.utils.enviroment import get_config
from src.utils import log


def analyze_cluster_distribution(mapping_df):
    """
    Analisa a distribui√ß√£o de tamanho dos clusters can√¥nicos
    
    Args:
        mapping_df: DataFrame com colunas [anonymized_listing_id, canonical_listing_id, listing_id_numeric]
    """
    log("\n" + "="*80)
    log("AN√ÅLISE DE DISTRIBUI√á√ÉO DE CLUSTERS")
    log("="*80)
    
    cluster_sizes = (
        mapping_df
        .groupBy("canonical_listing_id")
        .agg(
            F.count("anonymized_listing_id").alias("num_listings_agrupados"),
            F.countDistinct("anonymized_listing_id").alias("distinct_listings")
        )
    )
    
    # Estat√≠sticas descritivas
    stats = cluster_sizes.select("num_listings_agrupados").describe()
    log("\n Estat√≠sticas de Tamanho de Clusters:")
    stats.show()
    
    # Distribui√ß√£o por faixas
    distribution = (
        cluster_sizes
        .withColumn(
            "cluster_size_range",
            F.when(F.col("num_listings_agrupados") == 1, "1 (Singleton)")
            .when(F.col("num_listings_agrupados") <= 5, "2-5 (Pequeno)")
            .when(F.col("num_listings_agrupados") <= 10, "6-10 (M√©dio)")
            .when(F.col("num_listings_agrupados") <= 50, "11-50 (Grande)")
            .otherwise("50+ (Muito Grande)")
        )
        .groupBy("cluster_size_range")
        .count()
        .orderBy("cluster_size_range")
    )
    
    log("\n Distribui√ß√£o por Faixas de Tamanho:")
    distribution.show()
    
    # Top 10 maiores clusters
    log("\n Top 10 Maiores Clusters:")
    top_clusters = (
        cluster_sizes
        .orderBy(F.desc("num_listings_agrupados"))
        .limit(10)
    )
    top_clusters.show()
    
    return cluster_sizes


def calculate_sparsity_reduction(mapping_df, events_df=None):
    """
    Calcula a redu√ß√£o de esparsidade proporcionada pelo canonical_id
    
    Args:
        mapping_df: DataFrame de mapeamento
        events_df: DataFrame de eventos (opcional, para an√°lise de intera√ß√µes)
    """
    log("\n" + "="*80)
    log("AN√ÅLISE DE REDU√á√ÉO DE ESPARSIDADE")
    log("="*80)
    
    total_original_ids = mapping_df.select("anonymized_listing_id").distinct().count()
    total_canonical_ids = mapping_df.select("canonical_listing_id").distinct().count()
    
    reduction_pct = ((total_original_ids - total_canonical_ids) / total_original_ids) * 100
    
    log(f"\n Redu√ß√£o de IDs:")
    log(f"   - IDs Originais: {total_original_ids:,}")
    log(f"   - IDs Can√¥nicos: {total_canonical_ids:,}")
    log(f"   - Redu√ß√£o: {reduction_pct:.2f}%")
    
    if events_df:
        log("\n Impacto nas Intera√ß√µes:")
        
        # Intera√ß√µes por ID original
        original_interactions = (
            events_df
            .groupBy("anonymized_listing_id")
            .count()
            .select(F.avg("count").alias("avg_interactions_original"))
        )
        
        # Intera√ß√µes por ID can√¥nico
        # Nota: Se events_df n√£o tiver canonical_id, fazer JOIN
        # Se j√° tiver, usar diretamente
        canonical_interactions = (
            events_df
            .join(mapping_df.select("anonymized_listing_id", "canonical_listing_id"), 
                  on="anonymized_listing_id",
                  how="left")  # LEFT para manter todos eventos
            .groupBy("canonical_listing_id")
            .count()
            .select(F.avg("count").alias("avg_interactions_canonical"))
        )
        
        log("   M√©dia de Intera√ß√µes:")
        original_interactions.show()
        canonical_interactions.show()


def identify_problematic_clusters(mapping_df, listings_df):
    """
    Identifica clusters que podem estar agrupando itens muito diferentes
    
    Args:
        mapping_df: DataFrame de mapeamento
        listings_df: DataFrame processado de listings
    """
    log("\n" + "="*80)
    log("AN√ÅLISE DE QUALIDADE DOS CLUSTERS")
    log("="*80)
    
    # Juntar mapeamento com features originais
    # Nota: listings_df J√Å tem canonical_listing_id, ent√£o usar diretamente
    # sem necessidade de JOIN (evita duplica√ß√£o)
    enriched = listings_df
    
    # Clusters com alta varia√ß√£o de pre√ßo (indicativo de agrupamento incorreto)
    price_variance = (
        enriched
        .groupBy("canonical_listing_id")
        .agg(
            F.count("*").alias("num_listings"),
            F.stddev("price").alias("price_stddev"),
            F.avg("price").alias("avg_price"),
            F.min("price").alias("min_price"),
            F.max("price").alias("max_price")
        )
        .withColumn(
            "price_cv",  # Coeficiente de varia√ß√£o
            F.col("price_stddev") / F.col("avg_price")
        )
        .filter(F.col("num_listings") > 1)  # Apenas clusters com m√∫ltiplos itens
    )
    
    log("\nÔ∏è  Clusters com Alta Varia√ß√£o de Pre√ßo (CV > 0.3):")
    suspicious_clusters = (
        price_variance
        .filter(F.col("price_cv") > 0.3)
        .orderBy(F.desc("price_cv"))
        .limit(10)
    )
    suspicious_clusters.show()
    
    log("\n Clusters com Baixa Varia√ß√£o de Pre√ßo (CV < 0.1):")
    good_clusters = (
        price_variance
        .filter(F.col("price_cv") < 0.1)
        .orderBy("price_cv")
        .limit(10)
    )
    good_clusters.show()


def sample_cluster_inspection(mapping_df, listings_df, num_samples=5):
    """
    Inspeciona exemplos de clusters para valida√ß√£o manual
    
    Args:
        mapping_df: DataFrame de mapeamento
        listings_df: DataFrame processado de listings
        num_samples: N√∫mero de clusters a amostrar
    """
    log("\n" + "="*80)
    log("INSPE√á√ÉO DETALHADA DE CLUSTERS (AMOSTRA)")
    log("="*80)
    
    # Selecionar clusters de tamanho m√©dio (entre 2 e 10 itens)
    cluster_sizes = (
        mapping_df
        .groupBy("canonical_listing_id")
        .count()
        .filter((F.col("count") >= 2) & (F.col("count") <= 10))
    )
    
    sample_clusters = cluster_sizes.sample(False, 0.1).limit(num_samples)
    
    for row in sample_clusters.collect():
        canonical_id = row["canonical_listing_id"]
        size = row["count"]
        
        log(f"\n{'‚îÄ'*80}")
        log(f"Cluster ID: {canonical_id} | Tamanho: {size}")
        log(f"{'‚îÄ'*80}")
        
        # Buscar listings desse cluster
        # Filtrar diretamente por canonical_listing_id (j√° presente em listings_df)
        cluster_listings = (
            listings_df
            .filter(F.col("canonical_listing_id") == canonical_id)
            .select(
                "anonymized_listing_id",
                "lat_region", "lon_region", "zip_code",
                "usable_areas", "bedrooms", "suites", "unit_type",
                "price", "created_at"
            )
        )
        
        cluster_listings.show(truncate=False)


def export_validation_report(cluster_stats, output_path):
    """
    Exporta relat√≥rio de valida√ß√£o para CSV
    
    Args:
        cluster_stats: DataFrame com estat√≠sticas de clusters
        output_path: Caminho para salvar o relat√≥rio
    """
    log(f"\nüíæ Exportando relat√≥rio para: {output_path}")
    
    cluster_stats.coalesce(1).write.mode("overwrite").csv(
        output_path,
        header=True
    )
    
    log(" Relat√≥rio exportado com sucesso!")


def main():
    """Fun√ß√£o principal de valida√ß√£o"""
    spark = SparkSession.builder \
        .appName("Canonical ID Validation") \
        .getOrCreate()
    
    config = get_config()
    
    log(" Iniciando Valida√ß√£o do Canonical Listing ID\n")
    
    # Carregar dados
    log("üìÇ Carregando dados...")
    mapping_df = spark.read.parquet(config['raw_data']['listing_id_mapping_path'])
    listings_df = spark.read.parquet(config['raw_data']['listings_processed_path'])
    
    # Executar an√°lises
    cluster_stats = analyze_cluster_distribution(mapping_df)
    calculate_sparsity_reduction(mapping_df)
    identify_problematic_clusters(mapping_df, listings_df)
    sample_cluster_inspection(mapping_df, listings_df, num_samples=3)
    
    # Exportar relat√≥rio
    report_path = config['raw_data'].get('validation_report_path', 'data/reports/canonical_validation')
    export_validation_report(cluster_stats, report_path)
    
    log("\n Valida√ß√£o conclu√≠da!")
    
    spark.stop()


if __name__ == "__main__":
    main()
