model: BERT4Rec

n_layers: 2
n_heads: 2
hidden_size: 64
inner_size: 128

MAX_ITEM_LIST_LENGTH: 50

train_batch_size: 1024
eval_batch_size: 1024
learning_rate: 0.001

mask_ratio: 0.2

hidden_dropout_prob: 0.3
attn_dropout_prob: 0.3
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02

loss_type: 'CE'

# BERT4Rec usa masking - n√£o precisa de negative sampling
train_neg_sample_args: ~

clip_grad_norm:
  max_norm: 5.0
